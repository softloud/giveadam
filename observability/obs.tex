\documentclass{article}

% preamble
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{array}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{url}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}

% draft purposes
\usepackage{lineno}
\linenumbers
\usepackage{setspace}
\doublespacing


% hyperlinks
% filepath: /home/cantabile/Documents/repos/giveadam/observability/obs.tex
\usepackage{hyperref}
\definecolor{darkolive}{RGB}{85, 107, 47}
\hypersetup{
    colorlinks=true,
    linkcolor=darkolive,
    urlcolor=darkolive,
    citecolor=darkolive
}

% title
\title{Questionable analytical observations\\and what to do about them}
\author{Charles T. Gray \& Hannah Fraser}

\begin{document}

\maketitle



\begin{abstract}

A critical challenge in data management is balancing complex data transformation with the need for analytic transparency for domain-knowledge validation. FAIR (findable, accessible, interoperable, reusable) data principles as \textit{compliance} fall short when viewed as a checkbox exercise at publication. Living analyses evolve over time, and all analyses are iterative, requiring ongoing validation of data processing decisions. Contemporary data practices on the modern data stack provide robust tooling to validate data transformation, but in practice render data observability opaque to non-technical stakeholders. This creates a fundamental tension between engineering efficiency and analytic interpretability that undermines the core principles of FAIR data.

This document presents a novel tool-agnostic semantic layering methodology for analytic data pipeline design, demonstrated through provisioning of data comprising UN Sustainable Development Goal (SDG) priorities from surveys conducted in dam-affected communities in North India.

The central methodological contribution is a semantically-structured approach to data transformation that extends Data Build Tool's engineering-focused patterns to prioritize interpretability and stakeholder communication. Unlike standard data engineering implementations that emphasize technical efficiency, this semantic layering methodology explicitly preserves analytic context, makes harmonization decisions transparent, and maintains conceptual clarity throughout the data pipeline.

The semantic layer architecture consists of four conceptually distinct stages: source base (raw data cleaning), source entities (region-specific preservation), semantic models (cross-source harmonization), and analytic models (analysis-ready datasets). This approach addresses the critical gap between technical data engineering practices and analysis methodology requirements, enabling automated observability while maintaining scholarly rigor. We argue FAIR is as relevant and urgent for enterprise data engineering as it is for research questions.

This semantic layering methodology offers a replicable framework for data management that bridges the divide between engineering efficiency and analytic transparency, with applications across all domains requiring multi-source data integration and stakeholder communication.

\end{abstract}

\tableofcontents
\listoftables
\listoffigures


\section{Questionable analytical observations}

Underestimating the complexity of data transformation is a well-recognised driver of garbage in, garbage out analytics in scientific research, industry, and public institutions. For example, gene name errors caused by Excel formatting issues are widespread---detected in 700 scientific publications (approximately 20 per cent of 3,500 publications analysed) \cite{ziemann2016gene}. More recently, the UK government's COVID-19 tracking lost about 16,000 cases due to Excel row limits \cite{bbc2020excel}. These examples illustrate how technical limitations and data handling errors can lead to significant analytical inaccuracies.

\subsection{Questionable research practices}

Broadly, the purpose of research and analytics is to discover something useful about the world; to derive meaningful inference from data. However, in reality there are so many kinks in the pipeline between the research question and the answer, that the usability and usefulness of the answer that is produced by that pipeline is uncertain. Though, research that passes peer review is often seen as authoritative and accurate. The ‘replication crisis’ movement challenged this, finding that only 1/3 – 1/2 of research, when repeated, finds the same results. This low replication rate has been attributed in part to the use of \textbf{questionable research practices} which make research findings appear more impactful and reliable than warranted by the methods used (Gould et al. in prep). Examples of questionable research practices include HARKing (Hypothesising after results are known), p-hacking (which includes everything from choosing to drop outliers after examining the impact on the results, to rounding p-values down to reach a threshold e.g. p<0.05), and cherry picking (conducting multiple analyses but only presenting the ones with the significant/most publishable results) \cite{john2012measuring}. These practices are prevalent across the fields of science \cite{xie2021prevalence} are indicative of a huge problem in the research pipeline. However, the scope of QRP research has been limited to the analysis and write-up parts of the research pipeline. We argue that the focus on problematic research practices needs to include the earlier parts of the pipeline as well. In this paper we describe \textbf{questionable analytical observations} which occur between the point of data collection and the point of analysis, with flow on effects through the rest of the research pipeline. Questionable analytical observations arise when data transformation processes are not transparently validatable or interpretable to domain experts, leading to invisible garbage in, garbage out analytics. This is particularly problematic in multi-source data integration contexts, where harmonization decisions can significantly impact analytical outcomes. 

Sharing code, materials and data is one proposed counter to questionable research practices. This is in line with FAIR principles (see below), facilitates true critical analysis of the analysis methods, and should allow someone to test the method by using the code and data to reproduce the original results. Currently, we are falling well short when it comes to code and data sharing estimates suggesting that across fields 0.5-27 per cent of articles share code and 2-79 per cent share data \cite{hardwicke2018data, culina2020low, stodden2018empirical, kambouris2024computationally, hamilton2023prevalence}. Further, even if this code and data is provided, there is only around a 30 per cent chance that the same answers can be reproduced by a new analyst\cite{hardwicke2018data, kambouris2024computationally}. Clearly, there is a long way to go before an adequate level of transparency is achieved for the analysis component of the research pathway but at least the spotlight is on this issue. The parts of the pipeline between receiving rough-and-ready raw data and beginning the analysis have received almost no attention. In cases where the whole research pipeline is conducted by a single person, and that person is very diligent about documenting their code, methods and data, sometimes these steps are included. However, in other research contexts the research pipeline is carved up between researchers, and data engineers and curators. These circumstances require a premeditated structure to avoid questionable analytic observations that undermine the usefulness and reliability of their results. 

\subsection{What to do about questionable analytical observations}

To address questionable analytical observations, we propose a methodological bridge (Section~\ref{sec:semantic-methodology}) between modern data engineering practices and analytical validation. This involves:
\begin{itemize}
    \item \textbf{Semantic layering of data transformations} as shown in Figure~\ref{fig:architecture} to preserve analytic contexts and make harmonization decisions explicit (Section~\ref{subsec:core-innovation});
    \item \textbf{Automated observability frameworks} that generate transparent documentation and validation reports from pipeline artifacts (Section~\ref{subsec:obs-table-gen});
    \item \textbf{Stakeholder-centric communication} using conceptual rather than technical terminology to facilitate domain expert validation (Figures~\ref{fig:dbtdag} and \ref{fig:tehri-exclusions});
    \item \textbf{Iterative validation processes} that allow ongoing scrutiny of data processing decisions as analyses evolve over time.
\end{itemize}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/architecture.pdf}
  \caption{Data architecture: semantic layering from base to analytic models of \texttt{giveadam} data (described in Section~\ref{sec:case-study}), ordered by observability. This layering explictly delineates between loading the data (base), extracting source-specific entities (source entities), harmonizing across sources (semantic), and preparing analysis-ready datasets (analytic). Each layer serves a distinct purpose in the data transformation pipeline, enhancing transparency and interpretability for domain experts. This layering provides a domain and context agnostic minimal test-driven development architecture checking unique keys for missingness and duplicates.}
  \label{fig:architecture}
\end{figure}

\subsection{Case study: giveadam project}
\label{sec:case-study}

This manuscript and accompanying GitHub repository \href{https://github.com/softloud/giveadam}{\texttt{giveadam}} present a case study implementing the proposed methodology on open science data (Section~\ref{sec:aboutdata}). The project analyzes UN Sustainable Development Goal (SDG) priorities from dam-affected communities in North India, using survey data collected by Garima Gupta from residents of Tehri (where a dam was constructed 20 years ago) and Arunachal Pradesh (where a dam is under development).

This case study demonstrates the methodology on small, analytic data where validation is tractable, but the principles are applicable to large-scale data contexts. To illustrate scalability, the methodology uses open source modern data stack tools standard in enterprise environments, such as Data Build Tool (dbt). The following sections detail the semantic layering methodology, repository architecture, and adherence to FAIR principles.

\subsubsection{Minimal representation}

This case study was chosen as it provides a minimal representation of two key challenges in data managemen:

\begin{itemize}
  \item \textbf{Multi-source data integration:} The project integrates two regional survey datasets with differing formats and labeling conventions, requiring explicit harmonization decisions that impact analytical outcomes. 
  \item \textbf{Stakeholder alignment:} Interoperation of three people with different roles and expertise: Garima Gupta (data collection and domain expertise); Charles T. Gray (data engineering and pipeline design); and Hannah Fraser (code review and validation). Crucially, it is Garima Gupta who possessses the domain knowledge to validate the data processing decisions, but she is not a data engineer. This necessitates a methodology that facilitates transparent communication and validation across technical and domain expertise boundaries. Similarly, Hannah Fraser is not a data engineer, but she is a domain expert and can validate the data processing decisions are interpretable.
\end{itemize}

\subsection{Enterprise applications for data governance at scale}

The case study described above was chosen so that the methodology is interpretable on data that can be validated by eye. However, at scale, in large data platforms simply opening datasets is not an option. Platforms at scale for enterprise data management and public good typically involve:

\begin{itemize}
  \item hundreds, if not  \textbf{thousands, of sources} many of which are provisioned in legacy formats (e.g. Excel, CSV, XML, JSON, SQL dumps);
  \item hundreds, if not \textbf{thousands, of databases} requiring aggregation with integrity;
  \item \textbf{billions of rows of living data} per table in each database equiring ongoing validation specific to \texttt{snapshot} or \texttt{incremental} data processing.
\end{itemize}

When working across legacy sources in the volatility of the technology landscape, data engineers typically have little more to work with than inscrutable table names and column headers. Only after many complex steps requiring advanced software engineering codebases is the data available in a way that is accessible to stakeholders with enough domain knowledge to validate that what has been extracted by, say, ETL (extract, transform, load) pipelines is the desired output. 

\subsubsection{Modern data stack}

Advances in the modern data stack (a term to loosely define the 'collection of tools and cloud data technologies used to collect, process, store, and analyze data' \cite{prakashWhatDefinesModern2023}), such as dbt \cite{dbt_core} and dagster \cite{dagster} have unprecedented potential to address these challenges. However, there is a prevailing tendency for these tools to be oriented to interpretability by engineers, rendering the complexity of data processing --- crucially the assumptions made --- opaque to domain experts.

This creates a fundamental tension between engineering efficiency and analytic interpretability that undermines the core principles of FAIR data. The semantic layering methodology presented in Section~\ref{sec:semantic-methodology} addresses this tension by reorienting data transformation pipelines around research concepts rather than technical convenience, enabling both computational efficiency and methodological rigor.

\section{About the \texttt{giveadam} data}
\label{sec:aboutdata}

The data in this project aggregates two region-based surveys of UN SDG priorities from participants in Tehri and Arunachal Pradesh. Respondents ranked their top three UN SDG priorities, providing insights into development preferences in dam-affected communities. The data is stored in the \texttt{data/} directory of the giveadam repository (see Section~\ref{sec:repository-arch} for complete repository structure). This project aggregates survey responses across regions using the semantic layering methodology (detailed in Section~\ref{sec:semantic-methodology}) to investigate the following research question:

\begin{quote}
  \textit{What are the differences in SDG priorities between residents of Tehri, where a dam was constructed 20 years ago for hydroelectric power, and Arunachal Pradesh, where a dam is currently being developed?}
\end{quote}

Figure \ref{fig:top3-treemap} shows a treemap of the top 3 UN SDG priorities from survey respondents in Tehri (dam constructed 20 years ago) and Arunachal Pradesh (dam under development), North India.

\begin{figure}[ht]
  \centering
  %\includegraphics[width=0.95\textwidth]{../figures_and_tables/top3-treemap.png}
  \caption{\label{fig:top3-treemap} Treemap of top 3 UN SDG priorities from survey respondents in Tehri (dam constructed 20 years ago) and Arunachal Pradesh (dam under development), North India. Each rectangle represents a specific UN SDG, with its size proportional to the number of respondents who ranked it among their top three priorities. The treemap visually highlights the most and least prioritized SDGs in each region, providing insights into regional differences in development priorities.}
\end{figure}

By structuring the data in tidy format (one row per ranking by one respondent), the dataset enables flexible analysis across multiple dimensions of respondent characteristics and preferences. 

\section{Semantic data pipeline methodology}
\label{sec:semantic-methodology}

The core methodological innovation of the giveadam project is a semantic layering approach that reconceptualizes data transformation pipelines for research contexts. This methodology addresses a fundamental limitation in current data engineering practices: the tension between technical efficiency and research interpretability within the modern data stack paradigm \cite{prakashWhatDefinesModern2023}. Pipelines and the scale of 'big data' are becoming recognised as an integral component of reproducible analyses, however FAIRness is still framed from an extrinsic rather than intrinsic perspective \cite{leipzigRoleMetadataReproducible2021}. Furthermore, the structure of code and data is becoming recognised as a critical factor in ensuring the long-term usability and reproducibility of research outputs \cite{trisovicRepositoryApproachesImproving2021}. 

The focus in this manuscript is on embedding FAIR principles intrinsically within the data transformation process itself in recognition that right across the contexts of enterprise, large-scale public good, as well as extensible research requires transparency and accessibility of data transformation over time --- measured not in months, but \textit{decades}. If we are to trust data we must embrace it evolves over time with interoperators changing and data sources being added and removed. Toward an any analytic goal, we are now in a landscape of not \textit{data} but \textit{platforms that provision data} a diversity of people whose needs evolve, maintained by engineers who work in a fragile ecosystem where layoffs are commonplace.

Data engineering communities such as the vibrant open-source community developing dbt \cite{dbt_core} have made significant strides in democratizing data transformation through accessible tooling and best practices \cite{dbt_docs_structure}. However, these tools are primarily oriented toward engineering efficiency, often at the expense of interpretability for non-technical stakeholders. This creates a fundamental tension between engineering efficiency and analytic interpretability that undermines the core principles of FAIR data. 

Currently academic solutions to FAIR and enterprise engineering are evolving in disjunct worlds. We argue the solution lies in bridging the two worlds by reimagining robust techniques developed by engineers as FAIR to stakeholders. Duplicates, missingness, and scale are challenges in both research and enterprise contexts; there is no reason for advances in the modern data stack and FAIR research to be siloed.

\subsection{The semantic layering paradigm}
\label{subsec:semantic-paradigm}

Traditional data engineering prioritizes performance optimization and code maintainability, often at the expense of conceptual clarity for non-technical stakeholders. The semantic layering methodology inverts this priority structure, organizing transformations around research concepts rather than technical convenience. 

Furthermore, by prioritising a minimal test architecture that checks for uniqueness, duplicates, and missingness by semantic entities (responses and respondents in the \texttt{giveadam} case study discussed in Section~\ref{sec:case-study}) interpretable by domain experts, the semantic layering methodology ensures that data quality is validated in a manner aligned with analytic objectives as opposed to opaque technical metrics. Extracting these tests as observability provides a minimal viable data product that is extensible to attributes associated with the semantic entities as the analysis inevitably evolves through collaboration between data engineers and domain experts.

This approach ensures that:

\begin{enumerate}
    \item \textbf{Research context is preserved} throughout the transformation pipeline.
    \item \textbf{Harmonization decisions are explicit} and auditable by domain experts.
    \item \textbf{Stakeholder communication} uses conceptual rather than technical terminology.
    \item \textbf{Methodological transparency} is maintained without sacrificing automation.
\end{enumerate}

\subsection{Repository architecture supporting semantic design}
\label{sec:repository-arch}

The giveadam repository is organized into distinct functional areas:

\begin{itemize}
    \item \textbf{raw\_data/} — Original survey data files as provided by Garima Gupta
    \item \textbf{data/} — Published, analysis-ready datasets (pipeline outputs)
    \item \textbf{dbt\_project/} — Data transformation pipeline with semantic layering
    \item \textbf{observability/} — Automated methodology documentation and quality reports
    \item \textbf{scripts/} — Data preparation and analysis scripts (R, Python)
    \item \textbf{vis-scripts/} — Visualization generation scripts
\end{itemize}

\subsection{Core innovation: Semantic transformation architecture}
\label{subsec:core-innovation}

The central methodological contribution lies in reimagining data transformation layers as conceptual research stages rather than technical processing steps. Standard Data Build Tool implementations use staging $\to$ intermediate $\to$ marts layers optimized for engineering workflows \cite{dbt_docs_structure}. Layering data transformation is commendable for maintainability, however these terms are inscrutable to domain experts and obscure the analytic logic underpinning data processing.

The semantic layering methodology replaces these with four conceptually distinct stages (Figure~\ref{fig:architecture}) that separate  data transformations into layers interpretable to domain experts:

\begin{itemize}
    \item \textbf{source\_base/} — Raw data integrity and initial cleaning
    \item \textbf{source\_entities/} — Context-preserving regional data entities
    \item \textbf{semantic/} — Explicit cross-source harmonization with documented assumptions
    \item \textbf{analytic/} — Research-question-specific datasets ready for analysis
\end{itemize}

\subsubsection{Why semantic layering transforms research data management}
\label{subsubsec:semantic-benefits}

Each semantic layer addresses specific research methodology requirements:

\begin{enumerate}
    \item \textbf{Context preservation} — Source entities maintain regional survey characteristics, preventing premature harmonization that obscures methodological differences
    \item \textbf{Transparent harmonization} — Semantic models explicitly document how different data sources are reconciled (e.g., SDG labeling differences between regions)
    \item \textbf{Assumption visibility} — Every transformation decision is captured and testable, enabling methodological scrutiny
    \item \textbf{Stakeholder accessibility} — Layer terminology reflects research concepts (semantic, analytic) rather than engineering jargon (staging, marts)
    \item \textbf{Iterative extensibility} — New research questions can be addressed by extending the analytic layer without disrupting upstream logic
\end{enumerate}

This approach resolves the fundamental tension between automated data processing and research transparency, enabling both computational efficiency and methodological rigor.

\subsection{Automated observability for research transparency}

The observability framework generates this methodology documentation automatically from pipeline artifacts, ensuring that:
\begin{itemize}
    \item Documentation remains synchronized with actual transformations
    \item All data quality assumptions are explicitly validated and reported
    \item Research methodology is fully reproducible from code
    \item Stakeholders can verify data processing decisions without technical expertise
\end{itemize}

The complete automated documentation process includes both methodology generation and data publication. The validation tables in this document are generated through the automated process detailed in Section~\ref{subsec:obs-table-gen}. Additionally, \texttt{scripts/publish\_data.R} extracts all final analytic models from the dbt pipeline and exports them as CSV files to the \texttt{data/} directory for publication and external use.

The validation tables presented in this document are high-level summaries. Engineers or researchers requiring deeper pipeline investigation can use \texttt{dbt docs generate} and \texttt{dbt docs serve} for an interactive exploration (e.g., Figure~\ref{fig:tehri-exclusions}) of the complete data lineage (Figure~\ref{fig:dbtdag}), including detailed model specifications, column-level lineage, and test results. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{img/dbt-docs-tehri-exclusions.png}
  \caption{\label{fig:tehri-exclusions} Screenshot from interactive dbt documentation showing the SQL logic excluding two test rows from the Tehri dataset. This exclusion is fully documented in \texttt{dbt\_project/analyses/tehri\_rows\_excluded.sql} and reflects the methodological transparency principles outlined in Section~\ref{sec:semantic-methodology}. DBT documentation provides a user-friendly interface for engineers and stakeholders to verify data processing decisions without requiring technical expertise.}
\end{figure}

\section{FAIR principles implementation}
\label{sec:fair}

The giveadam project implements all four FAIR principles through its semantic layering methodology and automated observability framework. The semantic approach enhances traditional FAIR compliance by making research processes themselves findable, accessible, interoperable, and reusable. Complete technical details of the FAIR implementation, including dataset specifications, access protocols, and reusability frameworks, are provided in Appendix~A.

\section{Data lineage}
\label{sec:data-lineage}

The giveadam project pipeline transforms raw survey data through a systematic semantic layering approach, ensuring full traceability and methodological transparency. The complete data lineage, including technical pipeline details, model catalogs, validation frameworks, and directed acyclic graph visualizations, is documented in Appendix~B.

\section{Semantic layer implementation and observability}
\label{sec:semantic-implementation}

The observability framework operationalizes the semantic layering methodology through automated monitoring and validation at each conceptual stage. This implementation demonstrates how semantic design principles translate into technical infrastructure while maintaining research transparency. Complete implementation details, including technical infrastructure specifications, automated documentation processes, and validation frameworks, are provided in Appendix~C.

\section{Use of NLP tools}

The giveadam project architecture, semantic data pipeline design, and all data transformations were conceived and implemented by Charles T. Gray. GitHub Copilot \cite{copilot} was used for documentation editing and selected development operations tasks (supporting JSON manifest parsing scripts), but did not contribute to the fundamental design decisions, data modeling approach, or core analytical implementations. All code development, methodological innovations, and research insights represent the original work of the author.

\section{Acknowledgements}

The authors gratefully thanks Garima Gupta for collecting and sharing the original survey data from Tehri and Arunachal Pradesh, North India. The authors also acknowledge the open-source software community for developing the tools that made this project possible, including dbt, DuckDB, R, Python, and their associated libraries.

\appendix

\input{appendix_a_fair.tex}

\input{appendix_b_lineage.tex}

\input{appendix_c_implementation.tex}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
